{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales Artificiales\n",
    "\n",
    "Las redes neuronales artificiales (RNA) son un modelo de computación inspirado en el modelo biológico de redes neuronales.\n",
    "\n",
    "Estos modelos pueden ser aplicados en el contexto de aprendizaje mecánico, pues permiten descubrir patrones en conjuntos de datos que abstraen algún escenario particular. Posteriormente el modelo puede aplicar estos patrones para estimar valores o clasificaciones ante nuevas observaciones del mismo escenario.\n",
    "\n",
    "## Problema\n",
    "\n",
    "Vamos a conocer los detalles teóricos de modelamiento de RNA a través del desarrollo de un problema particular.\n",
    "\n",
    "Trabajaremos con un conjunto de datos para clasificación de frutas (https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/fruit_data_with_colors.txt). \n",
    "\n",
    "El conjunto de datos registra las medidas de masa, altura, anchura y color de 19 naranjas y 19 manzanas.\n",
    "\n",
    "Con este conjunto de datos y nuestra RNA realizaremos una tarea de clasificación, de manera tal que la RNA aprenderá a reconocer la fruta con base en sus medidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing as pre \n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga y limpieza de los datos\n",
    "fruits = pd.read_csv(\"https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/fruit_data_with_colors.txt\", sep='\\t')\n",
    "fruits = fruits[['fruit_name', 'mass', 'width', 'height', 'color_score']]\n",
    "fruits.columns = ['label', 'masa', 'ancho', 'alto', 'color']\n",
    "\n",
    "is_orange = fruits['label'] == 'orange'\n",
    "is_apple = fruits['label'] == 'apple'\n",
    "fruits = fruits[is_orange | is_apple]\n",
    "fruits['clase'] = fruits['label'] == 'orange'\n",
    "fruits = fruits.drop('label', axis=1)\n",
    "fruits['clase'] = fruits['clase'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masa</th>\n",
       "      <th>ancho</th>\n",
       "      <th>alto</th>\n",
       "      <th>color</th>\n",
       "      <th>clase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192</td>\n",
       "      <td>8.4</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>180</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>178</td>\n",
       "      <td>7.1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>172</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   masa  ancho  alto  color  clase\n",
       "0   192    8.4   7.3   0.55      0\n",
       "1   180    8.0   6.8   0.59      0\n",
       "2   176    7.4   7.2   0.60      0\n",
       "8   178    7.1   7.8   0.92      0\n",
       "9   172    7.4   7.0   0.89      0"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fruits\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de observaciones: 38\n"
     ]
    }
   ],
   "source": [
    "print('Número de observaciones:', len(fruits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuronas artificiales\n",
    "\n",
    "Las RNA están compuestas de tres tipos de unidades:\n",
    "\n",
    "+ Entradas: corresponden a los valores de entrada de las variables que modelan el problema.\n",
    "+ Neuronas: se conectan a las entradas, a otras neuronas, y a las salidas a través de sinapsis modeladas como pesos numéricos. Las neuronas realizan cálculos con sus valores de entrada para producir un valor de salida. La salida a su vez puede servir de entrada para subsecuentes neuronas o unidades de salida permitiendo la estructura reticular.\n",
    "+ Salidas: calculan un valor final para la RNA.\n",
    "\n",
    "![Neurona artificial](https://docs.google.com/drawings/d/e/2PACX-1vRB4UvKVoNPh3sbnu4U1rPLgTu_u6rBTK6bt_a04aOYn70bZ7b2zheOdA7dpIP2qZTGc_E2kgYTgAP4/pub?w=384&h=280)\n",
    "\n",
    "La susmación de actividad $s$ de una neurona se calcula sumando el producto de cada una de sus entradas $x_i$ por el correspondiente peso $p_i$ asignado a cada sinapsis.\n",
    "\n",
    "$$\n",
    "s = \\sum_{i=1}^{n} x_i p_i\n",
    "$$\n",
    "\n",
    "La activación de la neurona determinará cuál será su valor de salida. Es una función de la actividad de las entradas, de manera análoga al proceso de sumación temporal y el disparo de un potencial de acción en una neurona biológica. \n",
    "\n",
    "### Funciones de activación\n",
    "\n",
    "#### Sigmoide\n",
    "\n",
    "Definiremos a la función de activación $f_{act}(s)$ como la función *sigmoide* también llamada *logística*.\n",
    "\n",
    "$$\n",
    "f_{act}(s) = \\mathrm{sigmoide}(s) = \\frac{1}{1+\\mathrm{e}^{-s}}\n",
    "$$\n",
    "\n",
    "Para comprender el comportamiento de la función $\\mathrm{sigmoide}$, recordemos algunos límites:\n",
    "\n",
    "$$\n",
    "\\lim_{x \\to \\infty} \\mathrm{e}^{-x} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lim_{x \\to -\\infty} \\mathrm{e}^{-x} = \\infty\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lim_{x \\to \\infty} \\frac{1}{x} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathrm{e}^{0} = 1\n",
    "$$\n",
    "\n",
    "De esta forma tenemos que la función  $\\mathrm{sigmoide}$ se mueve en el rango $[0, 1]$ para argumentos en el rango $[-\\infty, \\infty]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fda2cf7e128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "x = np.arange(-10, 11, 0.01)\n",
    "plt.plot(x, sigmoide(x))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('sigmoide(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otras funciones típicamente utilizadas como funciones de activación son la tangente hiperbólica $tanh(x)$ y $step(x, \\Theta)$\n",
    "\n",
    "#### Tangente hiperbólica\n",
    "\n",
    "La función $tanh(x)$ se mueve en el rango $[-1, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fda2cf7e7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, np.tanh(x))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('tanh(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función *step*\n",
    "\n",
    "La función $step(x, \\Theta)$ recibe un argumento $\\Theta$ que delimita un umbral. Se define de la siguiente manera:\n",
    "\n",
    "$$\n",
    "step(x, \\Theta) = \n",
    "\\begin{cases}\n",
    "    1, x \\ge \\Theta \\\\\n",
    "    0, x < \\Theta\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fda2cf7ecc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "step = lambda x, theta: 1 if x >= theta else 0\n",
    "\n",
    "plt.plot(x, [step(xi, theta = 0.5) for xi in x])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('step(x, 0.5)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagación hacia adelante\n",
    "\n",
    "El algoritmo de propagación hacia adelante permite procesar las entradas, distribuyéndolas aritméticamente a través de todas las neuronas hasta finalmente producir una salida.\n",
    "\n",
    "Supongamos la siguiente arquitectura para la RNA que nos permitirá procesar nuestro ejemplo.\n",
    "\n",
    "![Propagación hacia adelante](https://docs.google.com/drawings/d/e/2PACX-1vSQSCKo8cHkbtuIqNW35LCczkk1qIk6qGLyFIqCWucl7CueVn2xRdRkkxGkJ2Q-pPtlpwvOzre-Jnxv/pub?w=549&h=625)\n",
    "\n",
    "Es importante recalcar que no hay un conjunto de reglas para determinar cuál debe ser la arquitectura de nuestra RNA. Esta arquitectura se escogió por su simplicidad, sin embargo no necesariamente será la óptima para modelar nuestro problema.¿Cómo seleccionar la arquitectura óptima? El número de entradas y de salidas viene determinado por el conjunto de datos que estemos utilizando. Sin embargo no hay reglas para definir el número de capas intermedias de neuronas, ni cuántas neuronas debe contener cada capa. Estos son parámetros de la construcción de la RNA; podemos definir sus valores a partir de la experiencia, de alguna hipótesis matemática basada en nuestro entendimiento del problema y del funcionamiento de las RNA, o puede ser producto de la aplicación de algún algoritmo de optimización como *simulated annealing* o algún algoritmo genético.\n",
    "\n",
    "Podemos representar las entradas, pesos y neuronas con matrices y de esta forma simplificar la presentación de los cálculos necesarios para realizar la propagación hacia adelante.\n",
    "\n",
    "Supongamos el siguiente subconjunto de datos simplificado de tan solo 3 observaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masa</th>\n",
       "      <th>ancho</th>\n",
       "      <th>alto</th>\n",
       "      <th>color</th>\n",
       "      <th>clase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>180</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>164</td>\n",
       "      <td>7.3</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>356</td>\n",
       "      <td>9.2</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    masa  ancho  alto  color  clase\n",
       "1    180    8.0   6.8   0.59      0\n",
       "13   164    7.3   7.7   0.70      0\n",
       "25   356    9.2   9.2   0.75      1"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[[1,13,25]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos representar $X$ como la matriz de entradas $3 \\times 4$, donde cada fila corresponde a una observación y cada columna a una variable del problema en el siguiente orden: `masa`, `ancho`, `alto`, `color`.\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "    180 & 8.0 & 6.8 & 0.59 \\\\\n",
    "    164 & 7.3 & 7.7 & 0.70 \\\\\n",
    "    356 & 9.2 & 9.2 & 0.75\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "En el caso de nuestro conjunto de datos completo la matriz tendría dimensiones $150 \\times 4$.\n",
    "\n",
    "Notemos que la última variable `clase` queda por fuera, pues correspondería al valor categórico $y$ que buscamos estimar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08333333  0.36842105  0.          0.        ]\n",
      " [ 0.          0.          0.375       0.6875    ]\n",
      " [ 1.          1.          1.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "X = data.loc[[1,13,25]][['masa', 'ancho', 'alto', 'color']].values\n",
    "\n",
    "scaler = pre.MinMaxScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "y = data.loc[[1,13,25]]['clase'].values.reshape(3,1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nota*: en el ejemplo anterior hicimos un paso de normalización de las entradas de $X$. Las RNA funcionan mejor cuando sus entradas están en el rango $[0, 1]$ o $[-1, 1]$. En el caso de nuestro conjunto de datos, como todas las variables corresponden a medidas de longitud podemos normalizarlas dividiendo por la máxima longitud disponible en los datos.\n",
    "\n",
    "**Ejercicio**: ¿Porqué las RNA funcionan mejor cuando sus entradas están en el rango $[0, 1]$ o $[-1,1]$? Investigue y trabaje una posible explicación, tome en cuenta los mecanismos de sumación de actividad y la función de activación.\n",
    "\n",
    "Los pesos de las sinapsis entre las entradas y la capa de neuronas oculta $P1$ los podemos modelar también como una matriz de dimensiones $4 \\times 5$. En esta matriz cada columna representa los pesos asociados entre las entradas y la neurona correspondiente; por ejemplo, la columna $p_{13}, p_{23}, p_{33}, p_{43}$ representa los pesos para la neurona 3. De manera más detallada, el elemento $p_{13}$ representa el peso de la sinapsis entre la variable 1 de la entrada $x_1$ y la neurona 3.\n",
    "\n",
    "$$\n",
    "P1 = \n",
    "\\begin{bmatrix}\n",
    "    p_{11} & p_{12} & p_{13} & p_{14} & p_{15} \\\\\n",
    "    p_{21} & p_{22} & p_{23} & p_{24} & p_{25} \\\\\n",
    "    p_{31} & p_{32} & p_{33} & p_{34} & p_{35} \\\\\n",
    "    p_{41} & p_{42} & p_{43} & p_{44} & p_{45}    \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Los valores para los pesos de las neuronas son inicialmente definidos aleatoriamente. Será posteriormente, en el paso de aprendizaje (propagación hacia atrás), que se calcularán los pesos finales del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.37454012  0.95071431  0.73199394  0.59865848  0.15601864]\n",
      " [ 0.15599452  0.05808361  0.86617615  0.60111501  0.70807258]\n",
      " [ 0.02058449  0.96990985  0.83244264  0.21233911  0.18182497]\n",
      " [ 0.18340451  0.30424224  0.52475643  0.43194502  0.29122914]]\n"
     ]
    }
   ],
   "source": [
    "P1 = np.random.rand(4,5)\n",
    "print(P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La sumación de actividad para la capa de neuronas oculta se puede definir entonces como una multiplicación de matrices.\n",
    "\n",
    "$$\n",
    "S2 = X \\cdot P1 =\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    0.08  p_{11} + 0.36  p_{21} + 0.0  p_{31} + 0.0  p_{41} & \\dots & \\\\\n",
    "    \\dots & \\dots \\\\\n",
    "    \\dots & 1  p_{15} + 1  p_{25} + 1  p_{35} + 1  p_{45} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "La matriz resultante $S2$ para nuestro subconjunto de ejemplo tendrá dimensiones $3 \\times 5$. Por otro lado, la matriz resultante $S2$ para el conjunto total tendría dimensiones $38 \\times 5$.\n",
    "\n",
    "Cada elemento de $S2$ representa la actividad generada por la observación en la neurona correspondiente. Por ejemplo $S2_{11}$ representa la sumación de actividad total de la observación 1 en la neurona 1; mientras que $S2_{34}$ representa la sumación de actividad total generada por la observación 3 en la neurona 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08868334  0.10062542  0.38011702  0.27135163  0.2738704 ]\n",
      " [ 0.13380979  0.57288274  0.67293604  0.37658937  0.2684044 ]\n",
      " [ 0.73452364  2.28295001  2.95536916  1.84405763  1.33714533]]\n"
     ]
    }
   ],
   "source": [
    "S2 = np.dot(X, P1)\n",
    "print(S2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez calculada la sumación de actividad, necesitamos determinar si  es suficiente para que nuestras neuronas generen su valor de salida. Con este fin aplicaremos la función de activación $f_{act}$ a cada elemento de $S2$.\n",
    "\n",
    "Recordemos que estaremos utilizando la función *sigmoide* como función de activación.\n",
    "\n",
    "$$\n",
    "f_{act}(s) = \\frac{1}{1 + \\mathrm{e}^{-s}}\n",
    "$$\n",
    "\n",
    "Al resultado de la aplicación de $f_{act}$ sobre $S2$ le llamaremos $A2$.\n",
    "\n",
    "$$\n",
    "A2 = f_{act}(S2) = \n",
    "\\begin{bmatrix}\n",
    "    f_{act}(S2_{11}) & f_{act}(S2_{12}) & f_{act}(S2_{13}) & f_{act}(S2_{14}) & f_{act}(S2_{15}) \\\\\n",
    "    f_{act}(S2_{21}) & f_{act}(S2_{22}) & f_{act}(S2_{23}) & f_{act}(S2_{24}) & f_{act}(S2_{25}) \\\\\n",
    "    f_{act}(S2_{31}) & f_{act}(S2_{32}) & f_{act}(S2_{33}) & f_{act}(S2_{34}) & f_{act}(S2_{35}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.52215632  0.52513515  0.59390133  0.5674247   0.56804283]\n",
      " [ 0.53340262  0.63942809  0.66216027  0.59305024  0.56670114]\n",
      " [ 0.67579717  0.90745509  0.95051664  0.86342789  0.7920201 ]]\n"
     ]
    }
   ],
   "source": [
    "A2 = sigmoide(S2)\n",
    "print(A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz $A2$ será a su vez la entrada para calcular el valor final de la estimación de $y$ que llamaremos $ŷ$. En una RNA con más de una capa de neuronas, la salida de la capa $A2$ será su vez la entrada para la siguiente capa, produciendo al final una nueva matriz $A3$ y así sucesivamente.\n",
    "\n",
    "Para calcular la sumación de actividad en la siguiente capa de salida recurrimos nuevamente a la multiplicación de matrices.\n",
    "\n",
    "$$\n",
    "S3 = A2 \\cdot P2\n",
    "$$\n",
    "\n",
    "Recordemos que $A2$ tiene dimensiones $3 \\times 5$; por otro lado $P2$ será una matriz de $5 \\times 1$ representando los pesos de las sinapsis entre la capa oculta y la unidad de salida.\n",
    "\n",
    "$$\n",
    "P2 = \n",
    "\\begin{bmatrix}\n",
    "    p_{11} \\\\\n",
    "    p_{21} \\\\\n",
    "    p_{31} \\\\\n",
    "    p_{41} \\\\\n",
    "    p_{51}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.61185289]\n",
      " [ 0.13949386]\n",
      " [ 0.29214465]\n",
      " [ 0.36636184]\n",
      " [ 0.45606998]]\n"
     ]
    }
   ],
   "source": [
    "P2 = np.random.rand(5,1)\n",
    "print(P2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz de sumación de actividad $S3$ tendrá entonces una dimensión $3 \\times 1$. Cada elemento de la matriz representa la sumación de actividad de las 5 neuronas de la capa oculta para cada observación.\n",
    "\n",
    "$$\n",
    "S3=\n",
    "\\begin{bmatrix}\n",
    "    A2_{11}p_{11} + A2_{12}p_{21} + A2_{13}p_{31} + A2_{14}p_{41} + A2_{15} p_{51}\\\\\n",
    "    A2_{21}p_{11} + A2_{22}p_{21} + A2_{23}p_{31} + A2_{24}p_{41} + A2_{25} p_{51}\\\\\n",
    "    A2_{31}p_{11} + A2_{32}p_{21} + A2_{33}p_{31} + A2_{34}p_{41} + A2_{35} p_{51}\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.03319112]\n",
      " [ 1.08473317]\n",
      " [ 1.49530484]]\n"
     ]
    }
   ],
   "source": [
    "S3 = np.dot(A2, P2)\n",
    "print(S3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente calculamos la activación de $S3$ a través de la aplicación de la función de activación a cada uno de sus elementos.\n",
    "\n",
    "$$\n",
    "\\hat{y} = f_{act}(S3) = \n",
    "\\begin{bmatrix}\n",
    "    f_{act}(S3_{11}) \\\\\n",
    "    f_{act}(S3_{21}) \\\\\n",
    "    f_{act}(S3_{31}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimado:\n",
      "[[ 0.73753409]\n",
      " [ 0.74738865]\n",
      " [ 0.81687317]]\n",
      "\n",
      "Esperado:\n",
      "[[0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "yhat = sigmoide(S3)\n",
    "print('Estimado:')\n",
    "print(yhat)\n",
    "print('\\nEsperado:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las estimaciones de nuestra RNA no son particularmente buenas. De hecho son estimaciones al azar, dada la iniciación aleatoria de las matrices de pesos $P1$ y $P2$.\n",
    "\n",
    "El aprendizaje en RNA se da a través del proceso de propagación hacia atrás, que ajusta los pesos con base en una tasa de error calculada entre la estimación $\\hat{y}$ y el valor real esperado $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagación hacia atrás\n",
    "\n",
    "### Tasa de error\n",
    "\n",
    "El proceso de propagación hacia atrás requiere que definamos una función de costo, esta función cuantificará la tasa de error en las estimaciones de $\\hat{y}$ con respecto al valor real esperado $y$ para cada una de nuestras observaciones.\n",
    "\n",
    "Definiremos nuestra función de costo de la siguiente manera:\n",
    "\n",
    "$$\n",
    "f_{costo} = \\sum_{i=1}^{obs} \\frac{1}{2} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Esta función calcula la suma total de las diferencias entre cada uno de los elementos de  la matriz $\\hat{y}$ y su correspondiente valor esperado en la matriz $y$. Conforme las estimaciones sean mejores la función de costo se irá acercando a cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcosto(y, yhat):\n",
    "    return np.sum(0.5 * (y - yhat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "costo: 0.568040882787\n"
     ]
    }
   ],
   "source": [
    "costo = fcosto(y, yhat)\n",
    "print('costo:', costo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descenso por gradiente\n",
    "\n",
    "El descenso por gradiente es un proceso de optimización que nos permitirá minimizar la función de costo, y por tanto minimizar el error de las estimaciones del modelo.\n",
    "\n",
    "La función de costo depende de $y$ y de $\\hat{y}$. La matriz $y$ la podemos considerar constante pues depende del conjunto de datos y no podemos manipularla. Por otro lado $\\hat{y}$ es función de $X$, $P1$ y $P2$; $X$ es también parte de nuestro conjunto de datos y tampoco podemos manipularla. De esta forma, nuestra estimación es producto de los pesos de la RNA. Los pesos si podemos modificarlos en busca de mejores estimaciones.\n",
    "\n",
    "Tenemos entonces una función de costo que es cóncava debido a su naturaleza cuadrática, y que es función tanto de P1 como P2. Nuestro objetivo es encontrar la combinación de P1 y P2 que minimicen el error.\n",
    "\n",
    "![Descenso](https://docs.google.com/drawings/d/e/2PACX-1vRUmNGppGsuEjs4QKhtwC1hSrdNBqz36jXoKwSjsFTTQbzGYDIQMCAxKIn5tbJEnzDKKRp8Nb2Wf_tV/pub?w=960&h=720)\n",
    "\n",
    "Recordemos que la derivada de una función permite determinar el cambio  de su valor con respecto a su variable independiente. La derivada se calcula como la pendiente de una recta tangente a la curva de la función. Utilizando la derivada de la función de costo podemos saber, para cualquier punto de la curva, si la pendiente desciende (cuando la derivada es negativa) o si la pendiente asciende (cuando es positiva). De esta forma podemos saber cómo debemos variar P1 y P2 para siempre descender hasta el mínimo global.\n",
    "\n",
    "![Derivada](https://docs.google.com/drawings/d/e/2PACX-1vRkGj6sCMhfUxdDMIOjtLU5ryDmv7BxFydq_FQtc-0s5q9Nv7GcrQJWuXheiyok2X037voMjGtRcBAw/pub?w=467&h=287)\n",
    "\n",
    "#### Derivada en $P2$\n",
    "\n",
    "Debemos entonces calcular la derivada de nuestra función de costo. Recordemos que $f_{costo}$ está definida en términos tanto de $P1$ como de $P2$, por tanto debemos calcular derivadas parciales $\\frac{\\partial f_{costo}}{\\partial P1}$ y $\\frac{\\partial f_{costo}}{\\partial P2}$ para ajustar cada matriz de pesos de manera individual.\n",
    "\n",
    "Empecemos por P2.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_{costo}}{\\partial P2} = \\frac{\\partial \\sum_{i=1}^{obs} \\frac{1}{2} (y_i - \\hat{y}_i)^2}{\\partial P2}\n",
    "$$\n",
    "\n",
    "Para simplificar el problema, podemos calcular la derivada para un sólo caso, y posteriormente sumar todas las derivadas valiéndonos de la regla de la suma de derivadas.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_{costo}}{\\partial P2} = \\frac{\\partial \\frac{1}{2} (y - \\hat{y})^2}{\\partial P2}\n",
    "$$\n",
    "\n",
    "Extraemos la constante.\n",
    "\n",
    "$$\n",
    "= \\frac{1}{2}  \\frac{\\partial  (y - \\hat{y})^2}{\\partial P2}\n",
    "$$\n",
    "\n",
    "Aplicamos regla de la cadena y simplificamos.\n",
    "\n",
    "$$\n",
    "= \\frac{1}{2} 2 (y - \\hat{y}) \\frac{\\partial (y - \\hat{y})}{\\partial P2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (y - \\hat{y}) \\frac{\\partial (y - \\hat{y})}{\\partial P2}\n",
    "$$\n",
    "\n",
    "Por regla de la suma.\n",
    "\n",
    "$$\n",
    "= (y - \\hat{y}) \\left( \\frac{\\partial y}{\\partial P2} - \\frac{\\partial \\hat{y}}{\\partial P2} \\right)\n",
    "$$\n",
    "\n",
    "Recordemos que $y$ se mantiene constante y no cambia en términos de $P2$, entonces por la regla de la constante.\n",
    "\n",
    "$$\n",
    "= (y - \\hat{y}) \\left( 0 - \\frac{\\partial \\hat{y}}{\\partial P2} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -(y - \\hat{y}) \\frac{\\partial \\hat{y}}{\\partial P2}\n",
    "$$\n",
    "\n",
    "En este punto, puesto que $\\hat{y} = f_{act}(S3)$ podemos aplicar nuevamente la regla de la cadena.\n",
    "\n",
    "$$\n",
    "= -(y - \\hat{y}) \\frac{\\partial \\hat{y}}{\\partial S3} \\frac{\\partial S3}{\\partial P2}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "= -(y - \\hat{y}) \\frac{\\partial f_{act}(S3)}{\\partial S3} \\frac{\\partial S3}{\\partial P2}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "= -(y - \\hat{y}) f^\\prime_{act}(S3) \\frac{\\partial S3}{\\partial P2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Hagamos un paréntesis para determinar $f^\\prime_{act}$.\n",
    "\n",
    "$$\n",
    "f_{act}(s) = \\frac{1}{1+\\mathrm{e}^{-s}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "f^\\prime_{act}(s) = \\frac{\\mathrm{d}f_{act}(s)}{\\mathrm{d}s} = \\frac{\\mathrm{d}}{\\mathrm{d}s} \\frac{1}{1+\\mathrm{e}^{-s}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{\\mathrm{d}}{\\mathrm{d}s} ({1+\\mathrm{e}^{-s}})^{-1}\n",
    "$$\n",
    "\n",
    "Aplicando regla de la cadena.\n",
    "\n",
    "$$\n",
    "= -({1+\\mathrm{e}^{-s}})^{-2} \\frac{\\mathrm{d}}{\\mathrm{d}s} \\left({1+\\mathrm{e}^{-s}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\frac{1}{({1+\\mathrm{e}^{-s}})^{2}} \\frac{\\mathrm{d}}{\\mathrm{d}s} \\left({1+\\mathrm{e}^{-s}}\\right)\n",
    "$$\n",
    "\n",
    "Regla de la suma.\n",
    "\n",
    "$$\n",
    "= -\\frac{1}{(1+\\mathrm{e}^{-s})^2} \n",
    "    \\left(\\frac{\\mathrm{d}}{\\mathrm{d}s} 1 +\n",
    "    \\frac{\\mathrm{d}}{\\mathrm{d}s} \\left(\\mathrm{e}^{-s}\\right) \\right)\n",
    "$$\n",
    "\n",
    "Regla de la constante.\n",
    "\n",
    "$$\n",
    "= -\\frac{1}{(1+\\mathrm{e}^{-s})^2} \n",
    "    \\left(0 +\n",
    "    \\frac{\\mathrm{d}}{\\mathrm{d}s} \\left(\\mathrm{e}^{-s}\\right) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\frac{1}{(1+\\mathrm{e}^{-s})^2} \\frac{\\mathrm{d}}{\\mathrm{d}s} \\mathrm{e}^{-s}\n",
    "$$\n",
    "\n",
    "Por regla de la cadena.\n",
    "\n",
    "$$\n",
    "= -\\frac{1}{(1+\\mathrm{e}^{-s})^2} \\left( \\mathrm{e}^{-s} \\frac{\\mathrm{d}}{\\mathrm{d}s}-s \\right)\n",
    "$$\n",
    "\n",
    "Sacando la constante.\n",
    "\n",
    "$$\n",
    "= -\\frac{1}{(1+\\mathrm{e}^{-s})^2} \\mathrm{e}^{-s} \\left(-\\frac{\\mathrm{d}}{\\mathrm{d}s}s \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\frac{1}{(1+\\mathrm{e}^{-s})^2} \\mathrm{e}^{-s} \\left(-1\\right)\n",
    "$$\n",
    "\n",
    "Simplificando.\n",
    "\n",
    "$$\n",
    "f^\\prime_{act}(s) = \\frac{\\mathrm{e}^{-s}}{(1+\\mathrm{e}^{-s})^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidePrima(s):\n",
    "    return np.exp(-s) / (1 + np.exp(-s)) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos ahora a la derivada parcial de nuestra función de costo en términos de $P2$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_{costo}}{\\partial P2} = -(y - \\hat{y}) f^\\prime_{act}(S3) \\frac{\\partial S3}{\\partial P2}\n",
    "$$\n",
    "\n",
    "Antes de resolver nuestra última derivada, recordemos que por simplicidad asumimos el supuesto de estar trabajando en una sola observación. En este punto debemos volver a tomar en cuenta la dimensionalidad de nuestras unidades. Para la primera parte de nuestra derivada parcial tenemos operaciones a nivel escalar.\n",
    "\n",
    "$$\n",
    "-(y - \\hat{y}) = -1 \\cdot\n",
    "\\begin{bmatrix}\n",
    "    y_{11} \\\\\n",
    "    y_{21} \\\\\n",
    "    y_{31} \\\\\n",
    "\\end{bmatrix} \n",
    "-\n",
    "\\begin{bmatrix}\n",
    "    \\hat{y}_{11} \\\\\n",
    "    \\hat{y}_{21} \\\\\n",
    "    \\hat{y}_{31}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    -y_{11} - \\hat{y}_{11} \\\\\n",
    "    -y_{21} - \\hat{y}_{21} \\\\\n",
    "    -y_{31} - \\hat{y}_{31}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Luego $-(y - \\hat{y}) \\cdot f^\\prime_{act}(S3)$ es una multiplicación escalar que llamaremos $\\Delta3$.\n",
    "\n",
    "$$\n",
    "\\Delta3 =\n",
    "\\begin{bmatrix}\n",
    "    -y_{11} - \\hat{y}_{11} \\\\\n",
    "    -y_{21} - \\hat{y}_{21} \\\\\n",
    "    -y_{31} - \\hat{y}_{31}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "    f^\\prime_{act}(S3_{11}) \\\\\n",
    "    f^\\prime_{act}(S3_{21}) \\\\\n",
    "    f^\\prime_{act}(S3_{31})\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    (-y_{11} - \\hat{y}_{11}) \\cdot f^\\prime_{act}(S3_{11}) \\\\\n",
    "    (-y_{21} - \\hat{y}_{21}) \\cdot f^\\prime_{act}(S3_{21}) \\\\\n",
    "    (-y_{31} - \\hat{y}_{31}) \\cdot f^\\prime_{act}(S3_{31}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Podemos entender a $\\Delta3$ como la cuantificación del error que se propaga desde la capa de unidades de salida de vuelta hacia la capa de neuronas intermedia.\n",
    "\n",
    "Podemos reformular entonces nuestra derivada parcial.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_{costo}}{\\partial P2} = \\Delta3 \\frac{\\partial S3}{\\partial P2}\n",
    "$$\n",
    "\n",
    "Y resolvemos el último término pendiente tomando en cuenta que $S3 = A2 \\cdot P2$.\n",
    "\n",
    "$$\n",
    "= \\Delta3 \\frac{\\partial A2 \\cdot P2}{\\partial P2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\Delta3 \\cdot A2\n",
    "$$\n",
    "\n",
    "Debemos entonces multiplicar el error por la activación en la capa intermedia. Si tomamos en cuenta la dimensionalidad de $\\Delta3$ $(3 \\times 1)$ y $A2$ $(3 \\times 5)$ notamos que para lograr esta multiplicación necesitamos transformar $A2$ en su transpuesta. \n",
    "\n",
    "$$\n",
    "= A2^T \\cdot \\Delta3\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    A2_{11} & A2_{21} & A2_{31} \\\\\n",
    "    A2_{12} & A2_{22} & A2_{32} \\\\\n",
    "    A2_{13} & A2_{23} & A2_{33} \\\\\n",
    "    A2_{14} & A2_{24} & A2_{34} \\\\\n",
    "    A2_{15} & A2_{25} & A2_{35}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "    \\Delta3_{11} \\\\\n",
    "    \\Delta3_{21} \\\\\n",
    "    \\Delta3_{31} \n",
    "\\end{bmatrix} \n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    A2_{11} \\Delta3_{11} + A2_{21} \\Delta3_{21} + A2_{31} \\Delta3_{31} \\\\\n",
    "    A2_{12} \\Delta3_{11} + A2_{22} \\Delta3_{21} + A2_{32} \\Delta3_{31} \\\\\n",
    "    A2_{13} \\Delta3_{11} + A2_{23} \\Delta3_{21} + A2_{33} \\Delta3_{31} \\\\  \n",
    "    A2_{14} \\Delta3_{11} + A2_{24} \\Delta3_{21} + A2_{34} \\Delta3_{31} \\\\  \n",
    "    A2_{15} \\Delta3_{11} + A2_{25} \\Delta3_{21} + A2_{35} \\Delta3_{31} \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Notemos que de esta forma cada fila corresponde a la suma de los errores de cada observación para cada una de las neuronas de la capa oculta; la matriz resultante tendrá dimensiones $(\\lvert capa\\ oculta \\rvert \\times \\lvert capa\\ salida\\rvert)$, es decir $5 \\times 1$ para nuestro caso de ejemplo. De esta forma logramos reintroducir además la sumatoria que eliminamos en el primer paso de nuestro proceso de derivación. \n",
    "\n",
    "Así finalmente obtenemos la definición de nuestra derivada parcial.\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_{costo}}{\\partial P2} = A2^T \\cdot \\Delta3\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.14277005]\n",
      " [ 0.14110612]\n",
      " [-0.0273942 ]]\n"
     ]
    }
   ],
   "source": [
    "Delta3 = -(y - yhat) * sigmoidePrima(S3)\n",
    "print(Delta3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.13130174]\n",
      " [ 0.14034178]\n",
      " [ 0.15218755]\n",
      " [ 0.14104135]\n",
      " [ 0.13936775]]\n"
     ]
    }
   ],
   "source": [
    "dfcosto_dP2 = np.dot(A2.T, Delta3)\n",
    "print(dfcosto_dP2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivada en $P1$\n",
    "\n",
    "Aún tenemos pendiente resolver nuestra otra derivada parcial en términos de $P1$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_{costo}}{\\partial P1} = \\frac{\\partial \\sum_{i=1}^{obs} \\frac{1}{2} (y_i - \\hat{y}_i)^2}{\\partial P1}\n",
    "$$\n",
    "\n",
    "Repitiendo los mismos pasos de la derivada anterior llegamos a este punto.\n",
    "\n",
    "$$\n",
    "= \\Delta3 \\frac{\\partial S3}{\\partial P1}\n",
    "$$\n",
    "\n",
    "Aplicando regla de la cadena.\n",
    "\n",
    "$$\n",
    "= \\Delta3 \\frac{\\partial S3}{\\partial A2} \\frac{\\partial A2}{\\partial P1}\n",
    "$$\n",
    "\n",
    "Expandiendo por $S3 = A2 \\cdot P2$.\n",
    "\n",
    "$$\n",
    "= \\Delta3 \\frac{\\partial A2 \\cdot P2}{\\partial A2} \\frac{\\partial A2}{\\partial P1}\n",
    "$$\n",
    "\n",
    "Tomando en cuenta la dimensionalidad de las matrices, utilizamos la transpuesta de $P2$. Otra manera de justificar la necesidad de transponer nuestras matrices implica tomar en cuenta que estamos realizando las operaciones en la dirección opuesta a la propagación hacia adelante.\n",
    "\n",
    "$$\n",
    "= \\Delta3 \\cdot P2^T \\frac{\\partial A2}{\\partial P1}\n",
    "$$\n",
    "\n",
    "Expandiendo por $A2 = f_{act}(S2)$.\n",
    "\n",
    "$$\n",
    "= \\Delta3 \\cdot P2^T \\frac{\\partial f_{act}(S2)}{\\partial P1}\n",
    "$$\n",
    "\n",
    "Por regla de la cadena.\n",
    "\n",
    "$$\n",
    "= \\Delta3 \\cdot P2^T \\frac{\\partial f_{act}(S2)}{\\partial S2} \\frac{\\partial S2}{\\partial P1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\Delta3 \\cdot P2^T \\cdot f^\\prime_{act}(S2) \\frac{\\partial S2}{\\partial P1}\n",
    "$$\n",
    "\n",
    "Expandiendo por $S2 = X \\cdot P1$.\n",
    "\n",
    "$$\n",
    "= \\Delta3 \\cdot P2^T \\cdot f^\\prime_{act}(S2) \\frac{\\partial X \\cdot P1}{\\partial P1}\n",
    "$$\n",
    "\n",
    "$$\n",
    " = \\Delta3 \\cdot P2^T \\cdot f^\\prime_{act}(S2) \\cdot X^T\n",
    "$$\n",
    "\n",
    "Definimos $\\Delta2$.\n",
    "\n",
    "$$\n",
    "\\Delta2 = \\Delta3 \\cdot P2^T \\cdot f^\\prime_{act}(S2)\n",
    "$$\n",
    "\n",
    "Y simplificamos.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_{costo}}{\\partial P1} = X^T \\cdot \\Delta2\n",
    "$$\n",
    "\n",
    "**Ejercicio**: ¿Cómo podemos generalizar estas operaciones a arquitecturas de múltiples capas ocultas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02179568  0.0049663   0.0100596   0.01283859  0.01597682]\n",
      " [ 0.02148772  0.00453821  0.00922184  0.01247637  0.01580225]\n",
      " [-0.00367231 -0.00032092 -0.00037642 -0.00118347 -0.00205801]]\n"
     ]
    }
   ],
   "source": [
    "Delta2 = np.dot(Delta3, P2.T) * sigmoidePrima(S2)\n",
    "print(Delta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.85599842e-03   9.29426616e-05   4.61877958e-04  -1.13586728e-04\n",
      "   -7.26609154e-04]\n",
      " [  4.35768344e-03   1.50877496e-03   3.32974763e-03   3.54653728e-03\n",
      "    3.82818620e-03]\n",
      " [  4.38558922e-03   1.38091261e-03   3.08176798e-03   3.49517104e-03\n",
      "    3.86783359e-03]\n",
      " [  1.11005014e-02   2.79910313e-03   5.96359330e-03   7.39403784e-03\n",
      "    8.80603731e-03]]\n"
     ]
    }
   ],
   "source": [
    "dfcosto_dP1 = np.dot(X.T, Delta2)\n",
    "print(dfcosto_dP1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagación hacia atrás\n",
    "\n",
    "Las derivadas que recién calculamos nos permiten conocer cuál dirección es \"cuesta arriba\".\n",
    "\n",
    "Notemos que $\\frac{\\partial f_{costo}}{\\partial P2}$ tiene dimensiones $5 \\times 1$ igual que $P2$; de la misma forma $\\frac{\\partial f_{costo}}{\\partial P1}$ tiene dimensiones $4 \\times 5$ igual que $P1$. \n",
    "\n",
    "Si tomamos en cuenta también que las derivadas representan la pendiente de una tangente, si las multiplicamos por un escalar podemos definir un vector lineal que represente movimiento (hacia arriba o hacia abajo) en la curva cóncava de la función de costo. Llamaremos a este escalar $\\alpha$ y lo interpretaremos como el factor de velocidad de aprendizaje. Este es un hiperparámetro más de la RNA, un $\\alpha$ muy pequeño producirá desplazamientos pequeños hacia el mínimo global, mientras que un $\\alpha$ muy grande podría provocar que nos \"brinquemos\" el mínimo global.\n",
    "\n",
    "Podemos entonces ajustar los pesos sumando o restando los vectores de movimiento a las matrices de pesos.\n",
    "\n",
    "$$\n",
    "P1 \\leftarrow P1 \\pm \\alpha \\frac{\\partial f_{costo}}{\\partial P1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P2 \\leftarrow P2 \\pm \\alpha \\frac{\\partial f_{costo}}{\\partial P2}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustemos los pesos de la RNA y verifiquemos cómo varía la función de costo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si modificamos los pesos sumando el valor de las derivadas, el costo debería aumentar, pues nos movemos cuesta arriba, alejándonos del mínimo global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "costo original:  0.568040882787\n",
      "nuevo costo: 0.825250822036\n"
     ]
    }
   ],
   "source": [
    "# modificamos los pesos para alejarnos del mínimo global\n",
    "alpha = 3\n",
    "nuevo_P1 = P1 + alpha * dfcosto_dP1\n",
    "nuevo_P2 = P2 + alpha * dfcosto_dP2\n",
    "\n",
    "# propagación hacia adelante\n",
    "nuevo_yhat = sigmoide(np.dot(sigmoide(np.dot(X, nuevo_P1)), nuevo_P2))\n",
    "nuevo_costo = fcosto(y, nuevo_yhat)\n",
    "\n",
    "print('costo original: ', costo)\n",
    "print('nuevo costo:', nuevo_costo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otro lado, si modificamos los pesos **restando** el valor de las derivadas, el costo se reduce reflejando un acercamiento hacia el mínimo global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "costo original:  0.568040882787\n",
      "nuevo costo: 0.37379148423\n"
     ]
    }
   ],
   "source": [
    "# modificamos los pesos para acercarnos al mínimo global\n",
    "alpha = 3\n",
    "nuevo_P1 = P1 - alpha * dfcosto_dP1\n",
    "nuevo_P2 = P2 - alpha * dfcosto_dP2\n",
    "\n",
    "# propagación hacia adelante\n",
    "nuevo_yhat = sigmoide(np.dot(sigmoide(np.dot(X, nuevo_P1)), nuevo_P2))\n",
    "nuevo_costo = fcosto(y, nuevo_yhat)\n",
    "\n",
    "print('costo original: ', costo)\n",
    "print('nuevo costo:', nuevo_costo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestras predicciones $\\hat{y}$ deberían mejorar con respecto a nuestro primer intento, más bien aleatorio, que desarrollamos en la sección de propagación hacia adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.73753409]\n",
      " [ 0.74738865]\n",
      " [ 0.81687317]]\n"
     ]
    }
   ],
   "source": [
    "# intento aleatorio\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.46448992]\n",
      " [ 0.45383592]\n",
      " [ 0.4291541 ]]\n"
     ]
    }
   ],
   "source": [
    "# intento después de un paso de descenso por gradiente\n",
    "print(nuevo_yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos notar que los dos primeros elementos de la nueva estimación se acercan más al valor esperado $0$. Sin embargo, el tercer elemento más bien se alejó de su valor esperado $1$.\n",
    "\n",
    "Debemos tener presente que tan sólo realizamos un paso de descenso en gradiente. Si continuamos calculando pasos de descenso deberíamos ir obteniendo cada vez mejores estimaciones.\n",
    "\n",
    "Por esta razón, el entrenamiento de una RNA consiste en realizar repetidamente pasos de (1) estimación (con propagación hacia adelante) y (2) de ajuste (con propagación hacia atrás); ya sea a través de un número determinado de iteraciones, o hasta que haya una convergencia -repitiendo hasta que la función de costo deje de disminuir-.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "Modelamos entonces el concepto de RNA a través de elementos de álgebra lineal: matrices y vectores.\n",
    "\n",
    "Definimos la función de activación como la función $\\mathrm{sigmoide}$.\n",
    "\n",
    "$$\n",
    "f_{act}(s) = \\mathrm{sigmoide}(s) = \\frac{1}{1+\\mathrm{e}^{-s}}\n",
    "$$\n",
    "\n",
    "Aunque también es común utilizar $\\mathrm{tanh}$ y $\\mathrm{step}$.\n",
    "\n",
    "Definimos el proceso de *propagación hacia adelante*, que permite calcular estimaciones $\\hat{y}$ para las salidas esperadas $y$ con base en las entradas $X$, con las siguientes ecuaciones:\n",
    "\n",
    "Sumación de actividad en capa oculta:\n",
    "$$\n",
    "S2 = X \\cdot P1\n",
    "$$\n",
    "\n",
    "Activación en capa oculta:\n",
    "$$\n",
    "A2 = f_{act}(S2)\n",
    "$$\n",
    "\n",
    "Sumación de actividad en capa de salida:\n",
    "$$\n",
    "S3 = A2 \\cdot P2\n",
    "$$\n",
    "\n",
    "Activación y estimación final en capa de salida:\n",
    "$$\n",
    "\\hat{y} = f_{act}(S3) = f_{act}(f_{act}(X \\cdot P1) \\cdot P2)\n",
    "$$\n",
    "\n",
    "Para el proceso de *propagación hacia atrás*, que permite ajustar las matrices de pesos $P1$ y $P2$ para mejorar las estimaciones, definimos la función de costo como una medida del error en las estimaciones $\\hat{y}$.\n",
    "\n",
    "$$\n",
    "f_{costo} = \\sum_{i=1}^{obs} \\frac{1}{2} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Esta función será minimizada a través de un proceso de descenso por gradiente. Este proceso requiere desarrollar derivadas parciales de la función de costo en términos de cada una de las matrices de pesos en la RNA.\n",
    "\n",
    "Derivada de la función de activación:\n",
    "$$\n",
    "f^\\prime_{act}(s) = \\frac{\\mathrm{e}^{-s}}{(1+\\mathrm{e}^{-s})^2}\n",
    "$$\n",
    "\n",
    "Propagación del error desde la capa de salida a la capa oculta:\n",
    "$$\n",
    "\\Delta3 = -(y - \\hat{y}) \\cdot f^\\prime_{act}(S3)\n",
    "$$\n",
    "\n",
    "Derivada para pesos entre capa oculta y capa de salida:\n",
    "$$\n",
    "\\frac{\\partial f_{costo}}{\\partial P2} = A2^T \\cdot \\Delta3\n",
    "$$\n",
    "\n",
    "Propagación del error desde la capa oculta a la capa de entradas:\n",
    "$$\n",
    "\\Delta2 = \\Delta3 \\cdot P2^T \\cdot f^\\prime_{act}(S2)\n",
    "$$\n",
    "\n",
    "Derivada para pesos entre capa de entrada y capa oculta:\n",
    "$$\n",
    "\\frac{\\partial f_{costo}}{\\partial P1} = X^T \\cdot \\Delta2\n",
    "$$\n",
    "\n",
    "Finalmente, las derivadas de la función de costo se utilizan para ir ajustando los pesos en un proceso iterativo de descenso hasta un mínimo global. \n",
    "\n",
    "Ajuste de pesos $P1$:\n",
    "$$\n",
    "P1 \\leftarrow P1 \\pm \\alpha \\frac{\\partial f_{costo}}{\\partial P1}\n",
    "$$\n",
    "\n",
    "Ajuste de pesos $P2$:\n",
    "$$\n",
    "P2 \\leftarrow P2 \\pm \\alpha \\frac{\\partial f_{costo}}{\\partial P2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**: Implemente una clase en Python que modele una RNA, debe implementar una interfaz pública que contenga los métodos `entrenar()` y `predecir(nuevo_X)`. El método `predecir(nuevo_X)` recibe una matriz de valores de entrada y produce una matriz de estimaciones de salida $\\hat{y}$. Su clase además debe implementar métodos privados `_prop_adelante(X)` y `_prop_atras(X, y)`. Finalmente, su clase debe permitir cualquier arquitectura de RNA, es decir el número de capas ocultas y cuántas neuronas por capa oculta debe ser parametrizable. El número de entradas y salidas se definen a partir de las dimensiones de `X` y `y`; tanto la matriz de entradas `X` como la de salidas esperadas `y` son parámetros del constructor. Finalmente, el constructor también debe tomar en cuenta otros hiperparámetros como la velocidad de aprendizaje `alfa` y el número máximo de iteraciones para el entrenamiento `iter`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografía\n",
    "\n",
    "Harrington, P. (2012). Machine Learning in Action. Manning: NY\n",
    "\n",
    "Li, S. (2017). Solving A Simple Classification Problem with Python — Fruits Lovers’ Edition. Disponible en: https://towardsdatascience.com/solving-a-simple-classification-problem-with-python-fruits-lovers-edition-d20ab6b071d2\n",
    "\n",
    "Méndez, H. (2013). Cálculo diferencial. EUNED: San José.\n",
    "\n",
    "Rojas, R. (1996). Neural Networks: A Systematic Introduction. Springer-Verlag: Berlin \n",
    "\n",
    "Welch Labs (N.D.). Neural Networks Demystified. Disponible en: https://www.youtube.com/playlist?list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
